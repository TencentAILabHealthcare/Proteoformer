{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# π-Proteoformer PTM sites functional association prediction \n",
    "\n",
    "This notebook demonstrates how to use the pre-trained $\\pi$-Proteoformer model for predicting PTM sites functional association within the protein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the proteoformer package to path\n",
    "PROJECT_ROOT = Path(\"your/project/root\")\n",
    "sys.path.insert(0, str(PROJECT_ROOT  / \"pi-Proteoformer\" / \"src\"))\n",
    "\n",
    "# Import Proteoformer components\n",
    "from proteoformer.net import ProteoformerForEmbedding\n",
    "from proteoformer.tokenization import ProteoformerTokenizer\n",
    "from proteoformer.models import PTMFucntionalClassifier\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths configuration\n",
    "PRETRAINED_MODEL_PATH = 'pretrain_stage2_balance_1126/checkpoint-197000'\n",
    "\n",
    "# Path to the PTM sites functional association prediction checkpoint\n",
    "PTM_CHECKPOINT_PATH = 'PTM_sites_functional_association_prediction.pt'\n",
    "\n",
    "# Example test data path (optional)\n",
    "TEST_DATA_PATH = '/fold1/test.json'\n",
    "\n",
    "# Window size for extracting PTM embeddings (number of amino acids on each side of PTM)\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "# Maximum sequence length for tokenization\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "# Hidden dimension for the classifier\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# Dropout rate for the classifier\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Batch size for inference\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PTMFucntionalClassifier is imported from proteoformer.models\n",
    "# \n",
    "# Architecture:\n",
    "#   - Takes precomputed PTM embeddings (h1 and h2)\n",
    "#   - Creates enhanced pair-wise features: [h1, h2, |h1 - h2|, h1 ⊙ h2]\n",
    "#   - Passes through fully connected layers for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ptm_position(proteoform: str, ptm_marker: str) -> int:\n",
    "    \"\"\"\n",
    "    Parse PTM position from proteoform sequence.\n",
    "    \n",
    "    The proteoform sequence contains PTM markers in the format [PFMOD:xxx].\n",
    "    This function finds the position of the residue that is modified.\n",
    "    \n",
    "    Args:\n",
    "        proteoform: Sequence with PTM marked (e.g., \"MMNKLYIGN...K[PFMOD:535]SGY...\")\n",
    "        ptm_marker: PTM marker to find (e.g., \"PFMOD:535\")\n",
    "    \n",
    "    Returns:\n",
    "        Position of the PTM residue (0-indexed)\n",
    "    \n",
    "    Example:\n",
    "        >>> proteoform = \"MMNKLYIGNLSPAVTADDLRQLFGDRKLPLAGQVLLK[PFMOD:535]SGYAFVD...\"\n",
    "        >>> parse_ptm_position(proteoform, \"PFMOD:535\")\n",
    "        36  # K at position 37 (1-indexed), 36 (0-indexed)\n",
    "    \"\"\"\n",
    "    ptm_pattern = re.escape(ptm_marker)\n",
    "    match = re.search(rf'\\[{ptm_pattern}\\]', proteoform)\n",
    "    \n",
    "    if match:\n",
    "        # Get the sequence before the PTM marker\n",
    "        seq_before = proteoform[:match.start()]\n",
    "        # Remove all PTM markers from the sequence before to get clean position\n",
    "        clean_seq_before = re.sub(r'\\[PFMOD:\\d+\\]', '', seq_before)\n",
    "        return len(clean_seq_before) - 1  # -1 because the residue is the last character\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find PTM marker {ptm_marker} in proteoform\")\n",
    "\n",
    "\n",
    "def extract_ptm_embedding(\n",
    "    hidden_states: torch.Tensor,\n",
    "    ptm_position: int,\n",
    "    attention_mask: torch.Tensor,\n",
    "    window_size: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract embedding for a PTM site by mean pooling over the PTM residue and its flanking regions.\n",
    "    \n",
    "    This creates a context-aware embedding that captures information from the \n",
    "    local sequence environment around the PTM site.\n",
    "    \n",
    "    Args:\n",
    "        hidden_states: Hidden states from encoder (seq_len, hidden_dim)\n",
    "        ptm_position: Position of the PTM residue (0-indexed)\n",
    "        attention_mask: Attention mask indicating valid positions (seq_len,)\n",
    "        window_size: Number of amino acids on each side of PTM to include\n",
    "    \n",
    "    Returns:\n",
    "        PTM embedding vector (hidden_dim,)\n",
    "    \"\"\"\n",
    "    seq_len = hidden_states.shape[0]\n",
    "    \n",
    "    # Define the window around the PTM site\n",
    "    start_pos = max(0, ptm_position - window_size)\n",
    "    end_pos = min(seq_len, ptm_position + window_size + 2)  # +2 to include the PTM residue\n",
    "    \n",
    "    # Extract embeddings in the window\n",
    "    window_embeddings = hidden_states[start_pos:end_pos]  # (window_len, hidden_dim)\n",
    "    window_mask = attention_mask[start_pos:end_pos]        # (window_len,)\n",
    "    \n",
    "    # Mean pooling (only over valid positions)\n",
    "    if window_mask.sum() > 0:\n",
    "        masked_embeddings = window_embeddings * window_mask.unsqueeze(-1)\n",
    "        ptm_embedding = masked_embeddings.sum(dim=0) / window_mask.sum()\n",
    "    else:\n",
    "        # Fallback: use the PTM position embedding directly\n",
    "        ptm_embedding = hidden_states[ptm_position]\n",
    "    \n",
    "    return ptm_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(\n",
    "    proteoformer_checkpoint: str,\n",
    "    ptm_func_checkpoint: str,\n",
    "    device: torch.device,\n",
    "    hidden_dim: int = 256,\n",
    "    dropout: float = 0.3\n",
    ") -> Tuple[ProteoformerForEmbedding, PTMFucntionalClassifier, ProteoformerTokenizer]:\n",
    "    \"\"\"\n",
    "    Load the complete PTM functional association prediction model.\n",
    "    \n",
    "    This loads:\n",
    "    1. Proteoformer2 model for encoding protein sequences\n",
    "    2. PTM classifier for predicting functional association\n",
    "    3. Tokenizer for sequence tokenization\n",
    "    \n",
    "    Args:\n",
    "        proteoformer_checkpoint: Path to Proteoformer2 model checkpoint\n",
    "        ptm_func_checkpoint: Path to fine-tuned PTM classifier checkpoint\n",
    "        device: Torch device to load models on\n",
    "        hidden_dim: Hidden dimension for the classifier\n",
    "        dropout: Dropout rate for the classifier\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (proteoformer_model, ptm_classifier, tokenizer)\n",
    "    \"\"\"\n",
    "    # print(f\"Loading Proteoformer2 from {proteoformer_checkpoint}...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = ProteoformerTokenizer.from_pretrained(proteoformer_checkpoint)\n",
    "    \n",
    "    # Load Proteoformer2 backbone model\n",
    "    proteoformer_model = ProteoformerForEmbedding.from_pretrained(proteoformer_checkpoint)\n",
    "    proteoformer_model = proteoformer_model.to(device)\n",
    "    proteoformer_model.eval()\n",
    "    \n",
    "    # Get embedding dimension from model config\n",
    "    embedding_dim = proteoformer_model.config.hidden_size\n",
    "    print(f\"  - Model hidden size: {embedding_dim}\")\n",
    "    print(f\"  - Number of layers: {proteoformer_model.config.num_hidden_layers}\")\n",
    "    \n",
    "    # Create the PTM classifier using PTMFucntionalClassifier from proteoformer.models\n",
    "    # print(f\"Loading PTM classifier from {ptm_func_checkpoint}...\")\n",
    "    ptm_classifier = PTMFucntionalClassifier(\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    # Load the fine-tuned checkpoint\n",
    "    checkpoint = torch.load(ptm_func_checkpoint, map_location=device)\n",
    "    ptm_classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    ptm_classifier = ptm_classifier.to(device)\n",
    "    ptm_classifier.eval()\n",
    "    \n",
    "    # Print checkpoint information\n",
    "    if 'metrics' in checkpoint:\n",
    "        metrics = checkpoint['metrics']\n",
    "        print(f\"Checkpoint metrics:\")\n",
    "        print(f\"  - Accuracy: {metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"  - Precision: {metrics.get('precision', 'N/A'):.4f}\")\n",
    "        print(f\"  - Recall: {metrics.get('recall', 'N/A'):.4f}\")\n",
    "        print(f\"  - F1 Score: {metrics.get('f1', 'N/A'):.4f}\")\n",
    "        print(f\"  - AUC: {metrics.get('auc', 'N/A'):.4f}\")\n",
    "    \n",
    "    print(\"Models loaded successfully!\")\n",
    "    return proteoformer_model, ptm_classifier, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_pair(\n",
    "    proteoformer_model: ProteoformerForEmbedding,\n",
    "    ptm_classifier: PTMFucntionalClassifier,\n",
    "    tokenizer: ProteoformerTokenizer,\n",
    "    proteoform1: str,\n",
    "    ptm1_marker: str,\n",
    "    proteoform2: str,\n",
    "    ptm2_marker: str,\n",
    "    device: torch.device,\n",
    "    window_size: int = 10,\n",
    "    max_length: int = 1024,\n",
    "    threshold: float = 0.5\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Predict functional association for a single PTM pair.\n",
    "    \n",
    "    This function:\n",
    "    1. Parses PTM positions from proteoform sequences\n",
    "    2. Encodes sequences using Proteoformer\n",
    "    3. Extracts PTM site embeddings\n",
    "    4. Predicts functional association using the classifier\n",
    "    \n",
    "    Args:\n",
    "        proteoformer_model: Loaded Proteoformer model\n",
    "        ptm_classifier: Loaded PTM classifier\n",
    "        tokenizer: Loaded tokenizer\n",
    "        proteoform1: First proteoform sequence with PTM marker\n",
    "        ptm1_marker: PTM marker for first site (e.g., \"PFMOD:535\")\n",
    "        proteoform2: Second proteoform sequence with PTM marker\n",
    "        ptm2_marker: PTM marker for second site (e.g., \"PFMOD:21\")\n",
    "        device: Torch device\n",
    "        window_size: Window size for embedding extraction\n",
    "        max_length: Maximum sequence length\n",
    "        threshold: Classification threshold (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - probability: Association probability (0-1)\n",
    "        - prediction: Binary prediction (0 or 1)\n",
    "        - label: Human-readable prediction label\n",
    "    \"\"\"\n",
    "    proteoformer_model.eval()\n",
    "    ptm_classifier.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Parse PTM positions\n",
    "        ptm1_position = parse_ptm_position(proteoform1, ptm1_marker)\n",
    "        ptm2_position = parse_ptm_position(proteoform2, ptm2_marker)\n",
    "        \n",
    "        # Tokenize proteoforms\n",
    "        encoded1 = tokenizer(\n",
    "            [proteoform1],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        \n",
    "        encoded2 = tokenizer(\n",
    "            [proteoform2],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids1 = encoded1['input_ids'].to(device)\n",
    "        attention_mask1 = encoded1['attention_mask'].to(device)\n",
    "        input_ids2 = encoded2['input_ids'].to(device)\n",
    "        attention_mask2 = encoded2['attention_mask'].to(device)\n",
    "        \n",
    "        # Encode proteoforms using ProteoformerForEmbedding\n",
    "        outputs1 = proteoformer_model(\n",
    "            input_ids=input_ids1,\n",
    "            attention_mask=attention_mask1\n",
    "        )\n",
    "        hidden_states1 = outputs1.last_hidden_state[0]  # (seq_len, hidden_dim)\n",
    "        \n",
    "        outputs2 = proteoformer_model(\n",
    "            input_ids=input_ids2,\n",
    "            attention_mask=attention_mask2\n",
    "        )\n",
    "        hidden_states2 = outputs2.last_hidden_state[0]  # (seq_len, hidden_dim)\n",
    "        \n",
    "        # Extract PTM embeddings\n",
    "        ptm1_embedding = extract_ptm_embedding(\n",
    "            hidden_states1, ptm1_position, attention_mask1[0], window_size\n",
    "        ).unsqueeze(0)  # (1, hidden_dim)\n",
    "        \n",
    "        ptm2_embedding = extract_ptm_embedding(\n",
    "            hidden_states2, ptm2_position, attention_mask2[0], window_size\n",
    "        ).unsqueeze(0)  # (1, hidden_dim)\n",
    "        \n",
    "        # Predict functional association\n",
    "        logits = ptm_classifier(ptm1_embedding, ptm2_embedding)\n",
    "        probability = torch.sigmoid(logits).item()\n",
    "        prediction = 1 if probability >= threshold else 0\n",
    "    \n",
    "    return {\n",
    "        'probability': probability,\n",
    "        'prediction': prediction,\n",
    "        'label': 'Functionally Associated' if prediction == 1 else 'Not Associated'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type proteoformer2 to instantiate a model of type proteoformer. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "π-Proteoformer PTM Sites Functional Association Prediction\n",
      "================================================================================\n",
      "\n",
      "[Step 1] Loading models...\n",
      "  - Model hidden size: 1280\n",
      "  - Number of layers: 24\n",
      "Checkpoint metrics:\n",
      "  - Accuracy: 0.7179\n",
      "  - Precision: 0.7618\n",
      "  - Recall: 0.6443\n",
      "  - F1 Score: 0.6981\n",
      "  - AUC: 0.7957\n",
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"π-Proteoformer PTM Sites Functional Association Prediction\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[Step 1] Loading models...\")\n",
    "\n",
    "proteoformer_model, ptm_classifier, tokenizer = load_models(\n",
    "    proteoformer_checkpoint=PRETRAINED_MODEL_PATH,\n",
    "    ptm_func_checkpoint=PTM_CHECKPOINT_PATH,\n",
    "    device=DEVICE,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 2] Single pair prediction example...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Step 2] Single pair prediction example...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Example from the test data\n",
    "# This example shows K37 with acetylation (PFMOD:535) and Y40 with phosphorylation (PFMOD:21)\n",
    "example = {\n",
    "    \"proteoform1\": \"MMNKLYIGNLSPAVTADDLRQLFGDRKLPLAGQVLLK[PFMOD:535]SGYAFVDYPDQNWAIRAIETLSGKVELHGKIMEVDYSVSKKLRSRKIQIRNIPPHLQWEVLDGLLAQYGTVENVEQVNTDTETAVVNVTYATREEAKIAMEKLSGHQFENYSFKISYIPDEEVSSPSPPQRAQRGDHSSREQGHAPGGTSQARQIDFPLRILVPTQFVGAIIGKEGLTIKNITKQTQSRVDIHRKENSGAAEKPVTIHATPEGTSEACRMILEIMQKEADETKLAEEIPLKILAHNGLVGRLIGKEGRNLKKIEHETGTKITISSLQDLSIYNPERTITVKGTVEACASAEIEIMKKLREAFENDMLAVNQQANLIPGLNLSALGIFSTGLSVLSPPAGPRGAPPAAPYHPFTTHSGYFSSLYPHHQFGPFPHHHSYPEQEIVNLFIPTQAVGAIIGKKGAHIKQLARFAGASIKIAPAEGPDVSERMVIITGPPEAQFKAQGRIFGKLKEENFFNPKEEVKLEAHIRVPSSTAGRVIGKGGKTVNELQNLTSAEVIVPRDQTPDENEEVIVRIIGHFFASQTAQRKIREIVQQVKQQEQKYPQGVASQRSK\",\n",
    "    \"Residue1\": \"K37\",\n",
    "    \"PTM1\": \"PFMOD:535\",\n",
    "    \"proteoform2\": \"MMNKLYIGNLSPAVTADDLRQLFGDRKLPLAGQVLLKSGY[PFMOD:21]AFVDYPDQNWAIRAIETLSGKVELHGKIMEVDYSVSKKLRSRKIQIRNIPPHLQWEVLDGLLAQYGTVENVEQVNTDTETAVVNVTYATREEAKIAMEKLSGHQFENYSFKISYIPDEEVSSPSPPQRAQRGDHSSREQGHAPGGTSQARQIDFPLRILVPTQFVGAIIGKEGLTIKNITKQTQSRVDIHRKENSGAAEKPVTIHATPEGTSEACRMILEIMQKEADETKLAEEIPLKILAHNGLVGRLIGKEGRNLKKIEHETGTKITISSLQDLSIYNPERTITVKGTVEACASAEIEIMKKLREAFENDMLAVNQQANLIPGLNLSALGIFSTGLSVLSPPAGPRGAPPAAPYHPFTTHSGYFSSLYPHHQFGPFPHHHSYPEQEIVNLFIPTQAVGAIIGKKGAHIKQLARFAGASIKIAPAEGPDVSERMVIITGPPEAQFKAQGRIFGKLKEENFFNPKEEVKLEAHIRVPSSTAGRVIGKGGKTVNELQNLTSAEVIVPRDQTPDENEEVIVRIIGHFFASQTAQRKIREIVQQVKQQEQKYPQGVASQRSK\",\n",
    "    \"Residue2\": \"Y40\",\n",
    "    \"PTM2\": \"PFMOD:21\",\n",
    "    \"label\": 1  # Ground truth: these PTM sites are functionally associated\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTM Site 1: K37 with modification PFMOD:535\n",
      "PTM Site 2: Y40 with modification PFMOD:21\n",
      "Ground Truth: Functionally Associated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"PTM Site 1: {example['Residue1']} with modification {example['PTM1']}\")\n",
    "print(f\"PTM Site 2: {example['Residue2']} with modification {example['PTM2']}\")\n",
    "print(f\"Ground Truth: {'Functionally Associated' if example['label'] == 1 else 'Not Associated'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Results:\n",
      "  - Probability: 0.6270\n",
      "  - Prediction: Functionally Associated\n",
      "  - Correct: ✓\n"
     ]
    }
   ],
   "source": [
    "# Make prediction\n",
    "result = predict_single_pair(\n",
    "    proteoformer_model=proteoformer_model,\n",
    "    ptm_classifier=ptm_classifier,\n",
    "    tokenizer=tokenizer,\n",
    "    proteoform1=example['proteoform1'],\n",
    "    ptm1_marker=example['PTM1'],\n",
    "    proteoform2=example['proteoform2'],\n",
    "    ptm2_marker=example['PTM2'],\n",
    "    device=DEVICE,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"Prediction Results:\")\n",
    "print(f\"  - Probability: {result['probability']:.4f}\")\n",
    "print(f\"  - Prediction: {result['label']}\")\n",
    "print(f\"  - Correct: {'✓' if result['prediction'] == example['label'] else '✗'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi_proteoform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
