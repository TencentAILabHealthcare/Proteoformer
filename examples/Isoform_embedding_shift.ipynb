{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# π-Proteoformer Isoform Embedding Shift \n",
    "\n",
    "This script demonstrates how to use the pre-trained π-Proteoformer model\n",
    "to compute embedding shift between protein isoforms and their canonical sequences.\n",
    "\n",
    "The embedding shift is defined as the Cosine distance between two vectors:\n",
    "    shift = 1 - cosine_similarity(isoform_embedding, canonical_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Import Libraries and Setup Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the proteoformer package to path\n",
    "PROJECT_ROOT = Path(\"your/project/root\")\n",
    "sys.path.insert(0, str(PROJECT_ROOT  / \"pi-Proteoformer\" / \"src\"))\n",
    "\n",
    "# Import Proteoformer components\n",
    "from proteoformer.net import ProteoformerForEmbedding\n",
    "from proteoformer.tokenization import ProteoformerTokenizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths configuration\n",
    "PRETRAINED_MODEL_PATH = 'pretrain_stage2_balance_1126/checkpoint-197000'\n",
    "\n",
    "# FASTA file containing protein sequences\n",
    "FASTA_FILE = \"isoform_seq.fasta\"\n",
    "\n",
    "# Maximum sequence length for tokenization\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "# Batch size for inference\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Example protein IDs to demonstrate\n",
    "CANONICAL_ID = \"O15409\"  # FOXP2_HUMAN - Forkhead box protein P2\n",
    "ISOFORM_ID = \"O15409-8\"  # Isoform 8 of FOXP2\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta(fasta_file: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse FASTA file and return a dictionary mapping protein IDs to sequences.\n",
    "    \n",
    "    Args:\n",
    "        fasta_file: Path to FASTA file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping protein IDs (e.g., 'A0FGR8', 'A0FGR8-2') to sequences\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with open(fasta_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                # Save previous sequence\n",
    "                if current_id is not None:\n",
    "                    sequences[current_id] = ''.join(current_seq)\n",
    "                \n",
    "                # Parse new header: >sp|A0FGR8|ESYT2_HUMAN or >sp|A0FGR8-2|ESYT2_HUMAN\n",
    "                parts = line.split('|')\n",
    "                if len(parts) >= 2:\n",
    "                    current_id = parts[1]  # Extract protein ID (e.g., A0FGR8 or A0FGR8-2)\n",
    "                else:\n",
    "                    current_id = line[1:].split()[0]  # Fallback\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "        \n",
    "        # Save last sequence\n",
    "        if current_id is not None:\n",
    "            sequences[current_id] = ''.join(current_seq)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "\n",
    "def mean_pooling(hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply mean pooling over sequence length (excluding padding).\n",
    "    \n",
    "    Args:\n",
    "        hidden_states: Hidden states from model (batch_size, seq_len, hidden_size)\n",
    "        attention_mask: Attention mask (batch_size, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        Pooled embeddings (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "    sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "    sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def cosine_distance(x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine distance between two vectors.\n",
    "    \n",
    "    Cosine distance is defined as: 1 - cosine_similarity(x, y)\n",
    "    \n",
    "    Args:\n",
    "        x: First embedding vector\n",
    "        y: Second embedding vector\n",
    "    \n",
    "    Returns:\n",
    "        Cosine distance (float between 0 and 2)\n",
    "    \"\"\"\n",
    "    # Ensure vectors are 1D\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cos_sim = F.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0), dim=1).item()\n",
    "    \n",
    "    # Return cosine distance\n",
    "    return 1.0 - cos_sim\n",
    "\n",
    "\n",
    "def extract_embedding(\n",
    "    model: ProteoformerForEmbedding,\n",
    "    tokenizer: ProteoformerTokenizer,\n",
    "    sequence: str,\n",
    "    device: torch.device,\n",
    "    max_length: int = 1024\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract embedding for a single protein sequence.\n",
    "    \n",
    "    Args:\n",
    "        model: Pretrained ProteoformerForEmbedding model\n",
    "        tokenizer: ProteoformerTokenizer\n",
    "        sequence: Protein sequence (amino acid string)\n",
    "        device: Device to run inference on\n",
    "        max_length: Maximum sequence length for model input\n",
    "    \n",
    "    Returns:\n",
    "        Embedding tensor (hidden_size,)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Tokenize sequence\n",
    "        encoded = tokenizer(\n",
    "            [sequence],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "        attention_mask = encoded['attention_mask'].to(device)\n",
    "        \n",
    "        # Get embeddings from model\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Extract last hidden state\n",
    "        hidden_states = outputs.last_hidden_state  # (1, seq_len, hidden_size)\n",
    "        \n",
    "        # Apply mean pooling\n",
    "        embedding = mean_pooling(hidden_states, attention_mask)\n",
    "        \n",
    "    return embedding.squeeze(0).cpu()  # (hidden_size,)\n",
    "\n",
    "def get_canonical_id(isoform_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract canonical protein ID from isoform ID.\n",
    "    \n",
    "    Args:\n",
    "        isoform_id: Isoform ID (e.g., 'A0FGR8-2')\n",
    "    \n",
    "    Returns:\n",
    "        Canonical protein ID (e.g., 'A0FGR8')\n",
    "    \"\"\"\n",
    "    return isoform_id.split('-')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Define Model Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device\n",
    ") -> Tuple[ProteoformerForEmbedding, ProteoformerTokenizer]:\n",
    "    \"\"\"\n",
    "    Load the π-Proteoformer model for embedding extraction.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to model checkpoint\n",
    "        device: Torch device to load model on\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer)\n",
    "    \"\"\"\n",
    "    # print(f\"Loading π-Proteoformer from {checkpoint_path}...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = ProteoformerTokenizer.from_pretrained(checkpoint_path)\n",
    "    \n",
    "    # Load model\n",
    "    model = ProteoformerForEmbedding.from_pretrained(checkpoint_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Print model info\n",
    "    print(f\"  - Model hidden size: {model.config.hidden_size}\")\n",
    "    print(f\"  - Number of layers: {model.config.num_hidden_layers}\")\n",
    "    print(f\"  - Number of attention heads: {model.config.num_attention_heads}\")\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Define Embedding Shift Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_shift(\n",
    "    model: ProteoformerForEmbedding,\n",
    "    tokenizer: ProteoformerTokenizer,\n",
    "    canonical_seq: str,\n",
    "    isoform_seq: str,\n",
    "    device: torch.device,\n",
    "    max_length: int = 1024\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Compute embedding shift between a canonical protein and its isoform.\n",
    "    \n",
    "    The embedding shift is defined as the Cosine distance:\n",
    "        shift = 1 - cosine_similarity(isoform_embedding, canonical_embedding)\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded π-Proteoformer model\n",
    "        tokenizer: Loaded tokenizer\n",
    "        canonical_seq: Canonical protein sequence\n",
    "        isoform_seq: Isoform sequence\n",
    "        device: Torch device\n",
    "        max_length: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - canonical_embedding: Embedding vector for canonical sequence\n",
    "        - isoform_embedding: Embedding vector for isoform sequence\n",
    "        - cosine_similarity: Cosine similarity between embeddings\n",
    "        - cosine_distance: Cosine distance (embedding shift)\n",
    "        - euclidean_distance: Euclidean distance between embeddings\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Extracting canonical embedding...\")\n",
    "    canonical_emb = extract_embedding(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        sequence=canonical_seq,\n",
    "        device=device,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    print(\"Extracting isoform embedding...\")\n",
    "    isoform_emb = extract_embedding(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        sequence=isoform_seq,\n",
    "        device=device,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cos_sim = F.cosine_similarity(\n",
    "        canonical_emb.unsqueeze(0), \n",
    "        isoform_emb.unsqueeze(0), \n",
    "        dim=1\n",
    "    ).item()\n",
    "    \n",
    "    # Compute cosine distance (embedding shift)\n",
    "    cos_dist = 1.0 - cos_sim\n",
    "    \n",
    "    # Compute Euclidean distance\n",
    "    euclidean_dist = torch.norm(canonical_emb - isoform_emb).item()\n",
    "    \n",
    "    return {\n",
    "        'canonical_embedding': canonical_emb,\n",
    "        'isoform_embedding': isoform_emb,\n",
    "        'cosine_similarity': cos_sim,\n",
    "        'cosine_distance': cos_dist,\n",
    "        'euclidean_distance': euclidean_dist\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading π-Proteoformer Model\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type proteoformer2 to instantiate a model of type proteoformer. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Model hidden size: 1280\n",
      "  - Number of layers: 24\n",
      "  - Number of attention heads: 16\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Loading π-Proteoformer Model\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model, tokenizer = load_model(\n",
    "    checkpoint_path=PRETRAINED_MODEL_PATH,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Load Sequences from FASTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading Sequences from FASTA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Loading Sequences from FASTA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Parse FASTA file\n",
    "sequences = parse_fasta(FASTA_FILE)\n",
    "\n",
    "# Get our example sequences\n",
    "canonical_seq = sequences.get(CANONICAL_ID)\n",
    "isoform_seq = sequences.get(ISOFORM_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Single Pair Embedding Shift Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting canonical embedding...\n",
      "Extracting isoform embedding...\n"
     ]
    }
   ],
   "source": [
    "# Compute embedding shift\n",
    "result = compute_embedding_shift(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    canonical_seq=canonical_seq,\n",
    "    isoform_seq=isoform_seq,\n",
    "    device=device,\n",
    "    max_length=MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding Shift Results:\n",
      "  - Embedding dimension: 1280\n",
      "  - Cosine Similarity: 0.605798\n",
      "  - Cosine Distance (Embedding Shift): 0.394202\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nEmbedding Shift Results:\")\n",
    "print(f\"  - Embedding dimension: {result['canonical_embedding'].shape[0]}\")\n",
    "print(f\"  - Cosine Similarity: {result['cosine_similarity']:.6f}\")\n",
    "print(f\"  - Cosine Distance (Embedding Shift): {result['cosine_distance']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi_proteoform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
